{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6M7wxF4_pvo"
   },
   "source": [
    "# <center> Human-Robot Interaction: Dialogue System (Fine-Tuning and Evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yAUxI9VPpVGt"
   },
   "source": [
    "# 1. Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HCBCl6aDmAOZ"
   },
   "outputs": [],
   "source": [
    "# set this to True if you want to train a new bot\n",
    "RUN_TRAINING = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXuRTjrJo5vk"
   },
   "source": [
    "Use the [DialoGPT](https://github.com/microsoft/DialoGPT) small version (117M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FSvzC1j7_Tr8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import requirements\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# download pretrained models (once)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyDqC81VI1Tq"
   },
   "source": [
    "Use the [Empathic Dialogues](https://github.com/facebookresearch/EmpatheticDialogues) dataset\n",
    "\n",
    "Here we've already downloaded and preprocessed the dataset for you (stored in \"Support/Data/empatheticdialogues\").\n",
    "\n",
    "**Download the fine-tuned model [here](https://drive.google.com/file/d/1ZebPQikUkiJ5Lwa0DydlkPnQS4wnWQou/view?usp=sharing) (~440MB) and unzip it under your \"Support\" folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E-9y-7Ztv6GG"
   },
   "outputs": [],
   "source": [
    "# accessing support files and dataset\n",
    "import sys, os\n",
    "from Practical11_Support.helper import *\n",
    "\n",
    "data_dir = 'Practical11_Support/Data/empatheticdialogues/'\n",
    "model_dir = 'Practical11_Support/fine-tuned'\n",
    "\n",
    "download_fine_tuned(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Kj2BIaUiS71"
   },
   "source": [
    "# 2. Configuring the model\n",
    "\n",
    "Training functions are inside the helper script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jv9TXRvV1HIk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob, logging, os, pickle, random, re, torch, pandas as pd, numpy as np\n",
    "from typing import Dict, List, Tuple\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from pathlib import Path\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "# training arguments\n",
    "# more info see: https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments\n",
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.output_dir = model_dir # where to save the fine-tuned model\n",
    "        self.model_type = 'gpt2' # don't change\n",
    "        self.model_name_or_path = 'microsoft/DialoGPT-small' # don't change\n",
    "        self.config_name = 'microsoft/DialoGPT-small' # don't change\n",
    "        self.tokenizer_name = 'microsoft/DialoGPT-small' # don't change\n",
    "        self.cache_dir = 'cached' # save cache to working directory, can change to somewhere else\n",
    "        self.block_size = 16 # don't change\n",
    "        self.per_gpu_train_batch_size = 1 # can be changed, speed-related\n",
    "        self.gradient_accumulation_steps = 1 # can be changed\n",
    "        self.learning_rate = 5e-5 # can be changed\n",
    "        self.weight_decay = 0.0 # can be changed\n",
    "        self.adam_epsilon = 1e-8 # can be changed\n",
    "        self.max_grad_norm = 1.0 # can be changed\n",
    "        self.num_train_epochs = 5  # can be changed\n",
    "        self.max_steps = -1 # don't change\n",
    "        self.warmup_steps = 0 # don't change\n",
    "        self.logging_steps = 1000 # don't change\n",
    "        self.save_total_limit = None # don't change\n",
    "        self.seed = 42 # don't change\n",
    "        self.local_rank = -1 # don't change\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbUy-WRgb_fA"
   },
   "source": [
    "# 3. Training an empathic chatbot\n",
    "Fine-tuning the DialoGPT-small model with the Empathic Dialogues dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UkXyNeS_fxMy"
   },
   "outputs": [],
   "source": [
    "# data segments\n",
    "data_trn = data_dir + 'train_processed.csv' # training set for training the model\n",
    "data_val = data_dir + 'valid_processed.csv' # validation set for parameter tunning\n",
    "data_tst = data_dir + 'test_processed.csv' # testing set for evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1n_n_Jo1jq4H"
   },
   "source": [
    "Take a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OFZsicHfjpxz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_trn, error_bad_lines=False)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ApbF-p305CYv"
   },
   "source": [
    "Fine-tuning the DialoGPT-small model on the training set. After training is finished the fine-tuned model will be saved as \"fine-tuned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sfTdpQy-5D1n",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set RUN_TRAINING to True if you want to train a new bot\n",
    "if RUN_TRAINING:\n",
    "    # training with more data takes longer and might lead to an out-of-memory error\n",
    "    df_trn = pd.read_csv(data_trn, error_bad_lines=False, usecols=['utterance'], nrows=3000)\n",
    "    main(df_trn, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNfnTRpwujO8"
   },
   "source": [
    "# 4. Evaluating the chatbot\n",
    "## 4.1 Quantitative and objective evaluation\n",
    "\n",
    "We define two metrics to evaluate the generated responses:\n",
    "- Metric 1: BLEU score, which measures the lexical similarity between the generated responses and the human responses (ground-truth)\n",
    "- Metric 2: sentiment alignment score, which measures the emotional similarity between the generated responses and the human responses (ground-truth). **Fill in the TODO to compute the sentiment label (1-5 stars) of the ground-truth and generated responses (sent_true and sent_pred).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WN9n3m-9E47w"
   },
   "outputs": [],
   "source": [
    "# compute BLEU score and alignment of sentiment between ground-truth responses and generated responses\n",
    "# import requirements\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "# download pretrained models for sentiment classification\n",
    "sent_model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "sent_model = AutoModelForSequenceClassification.from_pretrained(sent_model_name)\n",
    "sent_tokenizer = AutoTokenizer.from_pretrained(sent_model_name)\n",
    "sent_classifier = pipeline('sentiment-analysis', model=sent_model, tokenizer=sent_tokenizer)\n",
    "\n",
    "def eval(groundtruth, pred_prompt, tokenizer, model, verbose=False):\n",
    "    args = Args()\n",
    "    set_seed(args.seed) # Set seed\n",
    "\n",
    "    sent_align = 0\n",
    "\n",
    "    # ground-truth responses\n",
    "    groundtruth.utterance = groundtruth.utterance.str[:].str.split(' ').tolist()\n",
    "\n",
    "    # generated responses\n",
    "    pred = []\n",
    "    for prom_input in tqdm(pred_prompt.prompt):\n",
    "        # encoding input\n",
    "        new_prom_input_ids = tokenizer.encode(prom_input + tokenizer.eos_token, return_tensors='pt')\n",
    "        bot_input_ids = new_prom_input_ids\n",
    "\n",
    "        # generate a response with beam search\n",
    "        # more about different generation methods: https://huggingface.co/blog/how-to-generate \n",
    "        chat_history_ids = model.generate(\n",
    "            bot_input_ids,\n",
    "            do_sample=True, \n",
    "            max_length=200,\n",
    "            num_beams=5, \n",
    "            no_repeat_ngram_size=2,\n",
    "            early_stopping=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "        # save generated response\n",
    "        response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "        pred.append(response.split(' '))\n",
    "\n",
    "        # set verbose to true if you want to look at the responses for debugging\n",
    "        if verbose:\n",
    "            print(\"prompt: \", prom_input)\n",
    "            print(\"generated response: \", response)\n",
    "\n",
    "    # compute BLEU score (unigram-only)\n",
    "    bleu = bleu_score(pred, groundtruth.utterance, max_n=1, weights=[1.0])\n",
    "\n",
    "    # compute sentiment alignment between ground-truth and generated response\n",
    "    sent_align = 0\n",
    "\n",
    "    for i in tqdm(range(len(groundtruth.utterance))):\n",
    "        #TODO: compute sentiment lable of the ground-truth and generated response-------------\n",
    "        sent_true = ''\n",
    "        sent_pred = ''\n",
    "        #ENDTODO -----------------------------------------------------------------------------\n",
    "\n",
    "        if sent_true == sent_pred:\n",
    "            sent_align += 1\n",
    "\n",
    "        # set verbose to true if you want to look at the sentiments for debugging\n",
    "        if verbose:\n",
    "            print(\"sentiment of generated response: \", sent_pred)\n",
    "            print(\"ground truth response: \", human_response)\n",
    "            print(\"sentiment of ground truth response: \", sent_true)\n",
    "            \n",
    "    sent_score = sent_align/len(groundtruth.utterance)\n",
    "\n",
    "\n",
    "    return bleu, sent_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WGKyw0GAEEdu"
   },
   "source": [
    "Test the performance of the original DialoGPT-small model on the Empathic Dialogues dataset before fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fY93EbbkEDef"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')\n",
    "model = AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-small')\n",
    "\n",
    "# evaluate on the first 10 rows of the validation set\n",
    "nrows=10\n",
    "groundtruth = pd.read_csv(data_val, error_bad_lines=False, usecols=['utterance'], nrows=nrows)\n",
    "pred_prompt = pd.read_csv(data_val, error_bad_lines=False, usecols=['prompt'], nrows=nrows)\n",
    "bleu_val, sent_score_val = eval(groundtruth, pred_prompt, tokenizer, model)\n",
    "\n",
    "print(\"Original DialoGPT-small model:\")\n",
    "print(\"BLEU score on the validation set is {:.3f}\".format(bleu_val))\n",
    "print(\"Sentiment alignment score on the validation set is {:.3f}\".format(sent_score_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7V622VVEsEt"
   },
   "source": [
    "Test the fine-tuned model on the Empathic Dialogues dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P0gVbcg5uX_I"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')\n",
    "model = AutoModelForCausalLM.from_pretrained(args.output_dir) # use fine-tuned model\n",
    "\n",
    "# evaluate on the first 10 rows of the validation set\n",
    "nrows=10\n",
    "groundtruth = pd.read_csv(data_val, error_bad_lines=False, usecols=['utterance'], nrows=nrows)\n",
    "pred_prompt = pd.read_csv(data_val, error_bad_lines=False, usecols=['prompt'], nrows=nrows)\n",
    "\n",
    "'''\n",
    "# if you want to evaluate your model on a different segement (e.g., 100-199), use the code below instead:\n",
    "groundtruth = pd.read_csv(data_val, error_bad_lines=False, usecols=['utterance'])\n",
    "groundtruth = groundtruth.iloc[100:199]\n",
    "groundtruth = groundtruth.reset_index(drop=True)\n",
    "pred_prompt = pd.read_csv(data_val, error_bad_lines=False, usecols=['prompt'])\n",
    "pred_prompt = pred_prompt.iloc[100:199]\n",
    "pred_prompt = pred_prompt.reset_index(drop=True)\n",
    "'''\n",
    "\n",
    "bleu_val, sent_score_val = eval(groundtruth, pred_prompt, tokenizer, model, verbose=False)\n",
    "\n",
    "print(\"Fine-tuned model:\")\n",
    "print(\"BLEU score on the validation set is {:.3f}\".format(bleu_val))\n",
    "print(\"Sentiment alignment score on the validation set is {:.3f}\".format(sent_score_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3U3M15UYlhnb"
   },
   "outputs": [],
   "source": [
    "# post-lecture quiz 4\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')\n",
    "model = AutoModelForCausalLM.from_pretrained(args.output_dir) # use fine-tuned model\n",
    "\n",
    "# evaluate on the first 10 rows of the test set\n",
    "nrows=10\n",
    "groundtruth = pd.read_csv(data_tst, error_bad_lines=False, usecols=['utterance'], nrows=nrows)\n",
    "pred_prompt = pd.read_csv(data_tst, error_bad_lines=False, usecols=['prompt'], nrows=nrows)\n",
    "bleu_tst, sent_score_tst = eval(groundtruth, pred_prompt, tokenizer, model)\n",
    "print(\"Fine-tuned model:\")\n",
    "print(\"BLEU score on the test set is {:.3f}\".format(bleu_tst))\n",
    "print(\"Sentiment alignment score on the test set is {:.3f}\".format(sent_score_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xYlHoEB5Jic"
   },
   "source": [
    "## 4.2 Qualitative and subjective evaluation\n",
    "\n",
    "Chat with the fine-tuned chatbot to see how the responses differ from the original DialoGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NkZ0yjsc5LX-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')\n",
    "model = AutoModelForCausalLM.from_pretrained(args.output_dir)\n",
    "\n",
    "history = True # toggle whether to include dialog history or not\n",
    "fixed = True # toggle whether or not you want the bot to generate the same response for the same output\n",
    "n_turns = 5 # number of user inputs (turns)\n",
    "\n",
    "# set a random seed if you want the bot to generate the same response for the same output\n",
    "if fixed:\n",
    "    set_seed(1234)\n",
    "\n",
    "# Let's chat\n",
    "for step in range(n_turns):\n",
    "    # encode user input\n",
    "    user_input = input(\"You: \")\n",
    "    new_user_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # append the new input to the chat history and respond to the whole history\n",
    "    if history:\n",
    "        bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "    # only respond to the current input\n",
    "    else:\n",
    "        bot_input_ids = new_user_input_ids\n",
    "\n",
    "    # generated a response\n",
    "    chat_history_ids = model.generate(\n",
    "        bot_input_ids,\n",
    "        do_sample=True, \n",
    "        max_length=1000,\n",
    "        top_k=50, \n",
    "        top_p=0.95,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # print bot response\n",
    "    print(\"Bot: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], \n",
    "                                            skip_special_tokens=True)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Practical11_Part2_DialoGPT_Finetuned_Solution.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
